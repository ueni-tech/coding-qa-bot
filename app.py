import gc
import os
import shutil
from pathlib import Path

import PyPDF2
import streamlit as st
import tiktoken
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from chromadb.config import Settings
from langchain_community.vectorstores import Chroma

# ---------------------------------------------------------------------
# 0. å®šæ•°ãƒ»è¨­å®š
# ---------------------------------------------------------------------
load_dotenv()

# NOTE
# é€šå¸¸"/"ã¯å‰²ã‚Šç®—ã®æ¼”ç®—å­ã ãŒpathlibãŒã€Œãƒ‘ã‚¹çµåˆæ¼”ç®—å­ã€ã¨ã—ã¦å†å®šç¾©ã—ã¦ã„ã‚‹
PERSIST_DIR = (
    Path(__file__).parent
    / "vectorstore"  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã«"vectorstore"(/vectorstore)ã¨ãªã‚‹
)

# NOTE
# ChromaDBç”¨ã®è¨­å®šã€‚ã“ã®è¨­å®šã‚’ä½¿ã£ã¦Langchainã®Chromaãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’åˆæœŸåŒ–ãƒ»æ°¸ç¶šåŒ–ã™ã‚‹
CHROMA_SETTINGS = Settings(
    allow_reset=True,
    is_persistent=True,
    persist_directory=str(PERSIST_DIR),
    anonymized_telemetry=False,
)


# ---------------------------------------------------------------------
# 1. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ---------------------------------------------------------------------
@st.cache_data
def extract_text_from_pdf(pdf_file):
    reader = PyPDF2.PdfReader(pdf_file)
    return "\n".join((page.extract_text() or "") for page in reader.pages)


@st.cache_data
def split_text(text: str, size=1000, overlap=200):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", " ", ""],
        length_function=len,
    )
    return splitter.split_text(text)


def count_tokens(text: str, model="gpt-3.5-turbo"):
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


@st.cache_resource
def init_embeddings():
    return OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )


# ---------------------------------------------------------------------
# 2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ æ“ä½œ
# ---------------------------------------------------------------------
def cleanup_dirs(keep_uuid: str | None):
    """
    persist_directory é…ä¸‹ã§keep_uuidä»¥å¤–ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã™ã‚‹ãŸã‚ã®é–¢æ•°

    Args:
        keep_uuid(str | None): æ®‹ã—ãŸã„ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®å…¥ã£ãŸãƒ•ã‚©ãƒ«ãƒ€å

    Return:
        None
    """
    if not PERSIST_DIR.exists():
        return
    for p in PERSIST_DIR.iterdir():
        if p.is_dir() and p.name != keep_uuid:
            shutil.rmtree(p)


def rebuild_vectorstore(chunks, embeds):
    """
    ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ—¢å­˜æ¥ç¶šã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®é–¢æ•°
    1) streamlitã§ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º / reset
    2) æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ¥ç¶šä½œæˆ
    3) å¤ã„uuidãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤
    """
    # 1. æ—¢å­˜æ¥ç¶šã‚’å®‰å…¨ã«é–‰ã˜ã‚‹
    vs_old = st.session_state.pop("vectorstore", None)
    if vs_old:
        try:
            vs_old._client.reset()
        except Exception:
            pass
        del vs_old
        gc.collect()

    # 2. æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ç”Ÿæˆ
    vs_new = Chroma.from_texts(
        texts=chunks, embedding=embeds, client_settings=CHROMA_SETTINGS
    )
    vs_new.persist()
    st.session_state.vectorstore = vs_new

    # 3. å¤ããªã£ãŸuuidãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤
    current_uuid = vs_new._collection.id
    cleanup_dirs(current_uuid)

    return vs_new


def load_vectorstore(embeds):
    if PERSIST_DIR.exists():
        return Chroma(
            persist_directory=str(PERSIST_DIR),
            embedding_function=embeds,
            client_settings=CHROMA_SETTINGS,
        )
    return None


# ---------------------------------------------------------------------
# 3. Streamlit UI
# ---------------------------------------------------------------------
def main():
    st.set_page_config(
        page_title="ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„QAãƒœãƒƒãƒˆ", page_icon="ğŸ¤–", layout="wide"
    )
    st.title("ğŸ¤– ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„QAãƒœãƒƒãƒˆ")
    embeds = init_embeddings()
    if not embeds:
        st.stop()

    # æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®èª­ã¿è¾¼ã¿
    if vs := load_vectorstore(embeds):
        st.session_state.vectorstore = vs
        st.success("âœ… æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ----------------------------------------------
    with st.sidebar:
        st.header("è¨­å®š")
        pdf = st.file_uploader("ğŸ“„ è¦ç´„ PDF", type="pdf")
        size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", 500, 2000, 1000, 100)
        over = st.slider("ãƒãƒ£ãƒ³ã‚¯é‡è¤‡", 0, 500, 200, 50)
        top_k = st.slider("æ¤œç´¢ä»¶æ•°", 1, 10, 3)

        if st.button("ğŸ’£ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒªã‚»ãƒƒãƒˆ"):
            if "vectorstore" in st.session_state:
                uuid_now = st.session_state.vectorstore._collection.id
                st.session_state.vectorstore._client.reset()
                cleanup_dirs(uuid_now)
                del st.session_state.vectorstore
            st.success("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
            st.rerun()

    # --- pdf ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç† ----------------------------------------------
    if pdf:
        with st.spinner("PDF è§£æä¸­..."):
            text = extract_text_from_pdf(pdf)
            chunks = split_text(text, size, over)
            rebuild_vectorstore(chunks, embeds)
            st.success(
                f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ§‹ç¯‰å®Œäº† | æ–‡å­—æ•° {len(text):,} | "
                f"ãƒãƒ£ãƒ³ã‚¯ {len(chunks)} | æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³ {count_tokens(text):,}"
            )

    # --- è³ªå• UI ----------------------------------------------
    st.subheader("è³ªå•")
    if "vectorstore" not in st.session_state:
        st.warning("ã¾ãšã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        return

    q = st.text_input("è³ªå•ã‚’å…¥åŠ›", placeholder="ä¾‹: Python ã®å¤‰æ•°å‘½åè¦å‰‡ã¯ï¼Ÿ")
    if st.button("æ¤œç´¢"):
        if not q:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        with st.spinner("æ¤œç´¢ä¸­â€¦"):
            res = st.session_state.vectorstore.similarity_search_with_score(q, k=top_k)
        if not res:
            st.info("é–¢é€£æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        for i, (doc, score) in enumerate(res, 1):
            with st.expander(f"{i}. score {score:.3f}"):
                st.write(doc.page_content)


if __name__ == "__main__":
    main()
