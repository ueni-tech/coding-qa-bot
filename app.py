import gc
import os
import shutil
from pathlib import Path

import PyPDF2
import streamlit as st
import tiktoken
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableSequence
from chromadb.config import Settings
from langchain_community.vectorstores import Chroma

# ---------------------------------------------------------------------
# 0. å®šæ•°ãƒ»è¨­å®š
# ---------------------------------------------------------------------
load_dotenv()

# IMPORTANT: 2025å¹´6æœˆç¾åœ¨ã®æœ€é©ãªãƒ¢ãƒ‡ãƒ«é¸æŠã«ã¤ã„ã¦
# - gpt-4o-mini: 128K contextã€è¦–è¦šå¯¾å¿œã€æœ€ã‚‚ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒè‰¯ã„
# - gpt-4.1-nano: 2025å¹´æœ€æ–°ã€1M contextã€æœ€å®‰ãƒ»æœ€é€Ÿ
# - gpt-4.1-mini: GPT-4oã‚ˆã‚Š83%å®‰ä¾¡ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼åŠåˆ†
# RAGã‚·ã‚¹ãƒ†ãƒ ã«ã¯è¤‡é›‘ãªæ¨è«–ãŒä¸è¦ãªãŸã‚ã€gpt-4o-miniãŒæœ€é©


# NOTE
# é€šå¸¸"/"ã¯å‰²ã‚Šç®—ã®æ¼”ç®—å­ã ãŒpathlibãŒã€Œãƒ‘ã‚¹çµåˆæ¼”ç®—å­ã€ã¨ã—ã¦å†å®šç¾©ã—ã¦ã„ã‚‹
PERSIST_DIR = (
    Path(__file__).parent
    / "vectorstore"  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã«"vectorstore"(/vectorstore)ã¨ãªã‚‹
)

# NOTE
# ChromaDBç”¨ã®è¨­å®šã€‚ã“ã®è¨­å®šã‚’ä½¿ã£ã¦Langchainã®Chromaãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’åˆæœŸåŒ–ãƒ»æ°¸ç¶šåŒ–ã™ã‚‹
CHROMA_SETTINGS = Settings(
    allow_reset=True,
    is_persistent=True,
    persist_directory=str(PERSIST_DIR),
    anonymized_telemetry=False,
)

ENBEDDING_MODEL = "text-embedding-3-small"

# RAGç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
RAG_PROMPT_TEMPLATE = """\
ã‚ãªãŸã¯å„ªç§€ãªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã§å®Ÿç”¨çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæŒ‡ç¤º:**
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ã€Œæä¾›ã•ã‚ŒãŸè¦ç´„æ–‡æ›¸ã«è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„
- ã‚³ãƒ¼ãƒ‰ã®ä¾‹ã‚„ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’å«ã‚ã¦ã€å®Ÿç”¨çš„ãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- å›ç­”ã¯æ—¥æœ¬èªã§è¡Œã„ã€ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„

**ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:**
{context}

**è³ªå•:**
{question}

**å›ç­”:**"""


# ---------------------------------------------------------------------
# 1. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ---------------------------------------------------------------------
@st.cache_data
def extract_text_from_pdf(pdf_file):
    reader = PyPDF2.PdfReader(pdf_file)
    return "\n".join((page.extract_text() or "") for page in reader.pages)


@st.cache_data
def split_text(text: str, size=1000, overlap=200):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", " ", ""],
        length_function=len,
    )
    return splitter.split_text(text)


def count_tokens(text: str, model="gpt-3.5-turbo"):
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


@st.cache_resource
def init_embeddings():
    return OpenAIEmbeddings(
        model=ENBEDDING_MODEL,
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )


@st.cache_resource
def init_llm():
    """ChatOpenAIãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°"""
    return ChatOpenAI(
        model="gpt-4o-mini",  # 2025å¹´ç¾åœ¨ã€æœ€ã‚‚ã‚³ã‚¹ãƒ‘ã«å„ªã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        temperature=0,
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )


def format_docs(docs):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨ã™ã‚‹é–¢æ•°"""
    return "\n\n".join(doc.page_content for doc in docs)


# TODO Runnableã¨LCELã®åŸºç¤ã«ã¤ã„ã¦èª¿ã¹ã‚‹
# https://claude.ai/share/0cd33d5c-2d8c-4520-b17a-f0c89cf42581
def create_rag_chain(vectorstore, llm):
    """RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    retriever = vectorstore.as_retriever(
        search_type="similarity", search_kwargs={"k": 3}
    )

    prompt = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)

    # NOTE
    # "|"æ¼”ç®—å­ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹
    # LangChain ã® Runnable ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ãŠã‚Šã€Linuxã®ã‚ˆã†ã«å·¦ã‹ã‚‰å³ã¸ãƒ‡ãƒ¼ã‚¿ã‚’æµã™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¡¨ç¾ã™ã‚‹
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain, retriever


# ---------------------------------------------------------------------
# 2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ æ“ä½œ
# ---------------------------------------------------------------------
def cleanup_dirs(keep_uuid: str | None):
    """
    persist_directory é…ä¸‹ã§keep_uuidä»¥å¤–ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã™ã‚‹ãŸã‚ã®é–¢æ•°

    Args:
        keep_uuid(str | None): æ®‹ã—ãŸã„ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®å…¥ã£ãŸãƒ•ã‚©ãƒ«ãƒ€å

    Return:
        None
    """
    if not PERSIST_DIR.exists():
        return
    for p in PERSIST_DIR.iterdir():
        if p.is_dir() and p.name != keep_uuid:
            shutil.rmtree(p)


def rebuild_vectorstore(chunks, embeds):
    """
    ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ—¢å­˜æ¥ç¶šã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®é–¢æ•°
    1) streamlitã§ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º / reset
    2) æ–°è¦ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ¥ç¶šä½œæˆ
    3) å¤ã„uuidãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤
    """
    # 1. æ—¢å­˜æ¥ç¶šã‚’å®‰å…¨ã«é–‰ã˜ã‚‹
    vs_old = st.session_state.pop("vectorstore", None)
    if vs_old:
        try:
            vs_old._client.reset()
        except Exception:
            pass
        del vs_old
        gc.collect()

    # 2. æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ç”Ÿæˆ
    vs_new = Chroma.from_texts(
        texts=chunks, embedding=embeds, client_settings=CHROMA_SETTINGS
    )
    vs_new.persist()
    st.session_state.vectorstore = vs_new

    # 3. å¤ããªã£ãŸuuidãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤
    current_uuid = vs_new._collection.id
    cleanup_dirs(current_uuid)

    return vs_new


def load_vectorstore(embeds):
    if PERSIST_DIR.exists():
        return Chroma(
            persist_directory=str(PERSIST_DIR),
            embedding_function=embeds,
            client_settings=CHROMA_SETTINGS,
        )
    return None


# ---------------------------------------------------------------------
# 3. Streamlit UI
# ---------------------------------------------------------------------
def main():
    st.set_page_config(
        page_title="ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„QAãƒœãƒƒãƒˆ", page_icon="ğŸ¤–", layout="wide"
    )
    st.title("ğŸ¤– ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„QAãƒœãƒƒãƒˆ")

    embeds = init_embeddings()
    llm = init_llm()

    if not embeds or not llm:
        st.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        st.info("`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã«`OPENAI_API_KEY`ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®èª­ã¿è¾¼ã¿
    if vs := load_vectorstore(embeds):
        st.session_state.vectorstore = vs
        st.success("âœ… æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ----------------------------------------------
    with st.sidebar:
        st.header("è¨­å®š")
        pdf = st.file_uploader("ğŸ“„ è¦ç´„ PDF", type="pdf")
        size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", 500, 2000, 1000, 100)
        over = st.slider("ãƒãƒ£ãƒ³ã‚¯é‡è¤‡", 0, 500, 200, 50)

        st.subheader("æ¤œç´¢è¨­å®š")
        top_k = st.slider("æ¤œç´¢ä»¶æ•°", 1, 10, 3)

        st.subheader("LLMè¨­å®š")
        model_choice = st.selectbox(
            "ãƒ¢ãƒ‡ãƒ«é¸æŠ",
            [
                "gpt-4o-mini",  # æœ€ã‚‚ã‚³ã‚¹ãƒ‘ã«å„ªã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼ˆæ¨å¥¨ï¼‰
                "gpt-4.1-nano",  # 2025å¹´æœ€æ–°ã€æœ€å®‰ãƒ»æœ€é€Ÿãƒ¢ãƒ‡ãƒ«
                "gpt-4.1-mini",  # GPT-4oã‚ˆã‚Š83%å®‰ä¾¡ã§é«˜æ€§èƒ½
                "gpt-4o",  # ãƒãƒ©ãƒ³ã‚¹é‡è¦–
                "gpt-4-turbo",  # é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
                "gpt-3.5-turbo",  # æ—§ä¸–ä»£ï¼ˆéæ¨å¥¨ï¼‰
            ],
            index=0,
            help="gpt-4o-miniãŒæœ€ã‚‚ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å„ªã‚Œã¦ã„ã¾ã™",
        )
        temperature = st.slider(
            "Temperature",
            0.0,
            2.0,
            0.0,
            0.1,
            help="Temperatureã¯ç”Ÿæˆã•ã‚Œã‚‹å›ç­”ã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™ã€‚å€¤ãŒé«˜ã„ã»ã©ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒå¢—ã—ã€ä½ã„ã»ã©æ±ºå®šçš„ãªï¼ˆå®‰å®šã—ãŸï¼‰å›ç­”ã«ãªã‚Šã¾ã™ã€‚é€šå¸¸ã¯0ã€œ1ã®ç¯„å›²ã§èª¿æ•´ã—ã¾ã™ã€‚",
        )

        if st.button("ğŸ”„ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ›´æ–°"):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
            if "llm" in st.session_state:
                del st.session_state.llm

            st.session_state.llm = ChatOpenAI(
                model=model_choice,
                temperature=temperature,
                openai_api_key=os.getenv("OPENAI_API_KEY"),
            )

            # RAGãƒã‚§ãƒ¼ãƒ³ã‚‚å†æ§‹ç¯‰ãŒå¿…è¦
            if "rag_chain" in st.session_state:
                del st.session_state.rag_chain
            if "retriever" in st.session_state:
                del st.session_state.retriever

            st.success(f"âœ… ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ›´æ–°ã—ã¾ã—ãŸ: {model_choice}")

        if st.button("ğŸ’£ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒªã‚»ãƒƒãƒˆ"):
            if "vectorstore" in st.session_state:
                uuid_now = st.session_state.vectorstore._collection.id
                st.session_state.vectorstore._client.reset()
                cleanup_dirs(uuid_now)
                del st.session_state.vectorstore
                if "rag_chain" in st.session_state:
                    del st.session_state.rag_chain
                if "retriever" in st.session_state:
                    del st.session_state.retriever
            st.success("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
            st.rerun()

    # --- pdf ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç† ----------------------------------------------
    if pdf:
        with st.spinner("PDF è§£æä¸­..."):
            text = extract_text_from_pdf(pdf)
            chunks = split_text(text, size, over)
            vs = rebuild_vectorstore(chunks, embeds)

            # RAGãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
            current_llm = st.session_state.get("llm", llm)
            rag_chain, retriever = create_rag_chain(vs, current_llm)
            st.session_state.rag_chain = rag_chain
            st.session_state.retriever = retriever

            st.success(
                f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æ§‹ç¯‰å®Œäº† | æ–‡å­—æ•° {len(text):,} | "
                f"ãƒãƒ£ãƒ³ã‚¯ {len(chunks)} | æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³ {count_tokens(text):,}"
            )

    # --- è³ªå• UI ----------------------------------------------
    st.subheader("è³ªå•")
    if "vectorstore" not in st.session_state:
        st.warning("ã¾ãšã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        st.info(
            """
        **ä½¿ç”¨æ–¹æ³•:**
        1. ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        2. ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚„æ¤œç´¢è¨­å®šã‚’èª¿æ•´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        3. ä¸‹è¨˜ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«è³ªå•ã‚’å…¥åŠ›
        4. ã€ŒğŸ” RAGã§å›ç­”ç”Ÿæˆã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        """
        )
        return

    # è³ªå•å…¥åŠ›ã‚¨ãƒªã‚¢
    col1, col2 = st.columns([3, 1])

    with col1:
        question = st.text_input(
            "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            placeholder="ä¾‹: ã‚¯ãƒ©ã‚¹ã®å‘½åè¦å‰‡ã¯ï¼Ÿ",
            key="question_input",
        )

    with col2:
        search_mode = st.radio("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ["ğŸ” RAGå›ç­”", "ğŸ“– æ¤œç´¢ã®ã¿"], index=0)

    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("å®Ÿè¡Œ", type="primary", use_container_width=True):
        if not question.strip():
            st.warning("âš ï¸ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return

        # RAGãƒã‚§ãƒ¼ãƒ³ã®æº–å‚™
        if "rag_chain" not in st.session_state:
            current_llm = st.session_state.get("llm", llm)
            rag_chain, retriever = create_rag_chain(
                st.session_state.vectorstore, current_llm
            )
            st.session_state.rag_chain = rag_chain
            st.session_state.retriever = retriever

        if search_mode == "ğŸ” RAGå›ç­”":
            # RAGã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆ
            with st.spinner("ğŸ¤– AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­..."):
                try:
                    rag_chain = st.session_state.rag_chain
                    # TODO
                    # invoke()ã¨ã¯ãªã«ï¼Ÿ
                    answer = rag_chain.invoke(question)

                    # é–¢é€£æ–‡æ›¸ã‚‚å–å¾—ã—ã¦è¡¨ç¤º
                    retriever = st.session_state.retriever
                    docs = retriever.invoke(question)

                    # å›ç­”ã‚’è¡¨ç¤º
                    st.subheader("ğŸ¤– AIå›ç­”")
                    st.write(answer)

                    # å‚ç…§å…ƒæ–‡æ›¸ã‚’è¡¨ç¤º
                    with st.expander("ğŸ“š å‚ç…§ã—ãŸæ–‡æ›¸", expanded=False):
                        for i, doc in enumerate(docs, 1):
                            st.write(f"**æ–‡æ›¸ {i}:**")
                            st.write(doc.page_content)
                            st.divider()

                except Exception as e:
                    st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    st.info(
                        """
                        **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:**
                        - OpenAI APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                        - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª
                        - é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼ˆ{model}ï¼‰ãŒAPIå¯¾å¿œã‹ç¢ºèª
                        - ä½¿ç”¨é‡åˆ¶é™ã«é”ã—ã¦ã„ãªã„ã‹ç¢ºèª
                        """.format(
                            model=st.session_state.get("llm", llm).model_name
                        )
                    )
        else:
            # æ¤œç´¢ã®ã¿ãƒ¢ãƒ¼ãƒ‰
            with st.spinner("ğŸ” é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ä¸­..."):
                retriever = st.session_state.get("retriever")
                if not retriever:
                    current_llm = st.session_state.get("llm", llm)
                    _, retriever = create_rag_chain(
                        st.session_state.vectorstore, current_llm
                    )
                    st.session_state.retriever = retriever

                docs = retriever.invoke(question)

                if not docs:
                    st.info("ğŸ” é–¢é€£æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    return

                st.subheader("ğŸ” æ¤œç´¢çµæœ")
                for i, doc in enumerate(docs, 1):
                    with st.expander(f"ğŸ“„ æ–‡æ›¸ {i}", expanded=True):
                        st.write(doc.page_content)

    # --- çµ±è¨ˆæƒ…å ± ----------------------------------------------
    if "vectorstore" in st.session_state:
        with st.expander("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±", expanded=False):
            vs = st.session_state.vectorstore
            collection = vs._collection

            st.write("**ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢æƒ…å ±:**")
            st.write(f"- ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ID: `{collection.id}`")
            st.write(f"- ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {collection.count()}")
            st.write(f"- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: `{ENBEDDING_MODEL}`")

            if "rag_chain" in st.session_state:
                current_llm_model = st.session_state.get("llm", llm).model_name
                st.write("**RAGã‚·ã‚¹ãƒ†ãƒ :**")
                st.write("- çŠ¶æ…‹: âœ… æ­£å¸¸ç¨¼åƒä¸­")
                st.write(f"- ä½¿ç”¨ä¸­ã®LLMãƒ¢ãƒ‡ãƒ«: `{current_llm_model}`")
                st.write("- ãƒã‚§ãƒ¼ãƒ³: Retriever â†’ Prompt â†’ LLM â†’ Parser")
                st.write("- æœ€é©åŒ–: ã‚³ã‚¹ãƒˆåŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹")


if __name__ == "__main__":
    main()
