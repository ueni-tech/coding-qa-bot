import os
import shutil
import PyPDF2

import tiktoken
import streamlit as st
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma

load_dotenv()


@st.cache_data  # é–¢æ•°ã®æˆ»ã‚Šå€¤ï¼ˆPDFã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãŸã‚ã®ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def extract_text_from_pdf(pdf_file):
    """
    PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘å–ã‚Šãã®ä¸­ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã™ã‚‹é–¢æ•°

    Args:
        pdf_file: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        str: æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """

    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)

        text = ""
        for page_num in range(len(pdf_reader.pages)):
            page = pdf_reader.pages[page_num]
            text += page.extract_text() + "\n"

        return text

    except Exception as e:
        st.error(f"PDFã®èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None


@st.cache_data
def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=200):
    """
    å—ã‘å–ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã™ã‚‹é–¢æ•°

    Args:
        text(str): åˆ†å‰²å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size(int): å„ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§æ–‡å­—æ•°
        chunk_overlap(int): ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡æ–‡å­—æ•°

    Returns:
        list: åˆ†å‰²ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    try:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            # len(æ–‡å­—æ•°)ãƒ™ãƒ¼ã‚¹ã§ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’æŒ‡å®š
            length_function=len,
            separators=["\n\n", "\n", " ", ""],
        )

        chunks = text_splitter.split_text(text)

        return chunks

    except Exception as e:
        st.error(f"ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return []


def count_tokens(text, model="gpt-3.5-turbo"):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹é–¢æ•°

    Args:
        text(str): ã‚«ã‚¦ãƒ³ãƒˆå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        model(str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«

    Returns:
        int: ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    """
    # TODO
    # ãªãœã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹é–¢æ•°ãŒå¿…è¦ï¼Ÿ
    # â†’ APIåˆ¶é™ã€ã‚³ã‚¹ãƒˆè¦‹ç©ã€æ–‡è„ˆåˆ¶å¾¡ã®ãŸã‚

    try:
        encoding = tiktoken.encoding_for_model(model)
        return len(encoding.encode(text))
    except Exception as e:
        st.error(f"ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return 0


@st.cache_resource
def initialize_embeddings():
    """Open AI Embeddingsã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°(@st.cache_resourceã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥)"""
    try:
        # NOTE
        # api_keyã«ã¯å‹SecretStr | NoneãŒè¦æ±‚ã•ã‚Œã¦ã„ã‚‹ãŒos.getenv("OPEN_API_KEY")ã§ã„ã„ã¿ãŸã„
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small", openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        return embeddings
    except Exception as e:
        st.error(f"EmbeddingåˆæœŸåŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None


def create_vector_store(text_chunks, embeddings, persist_directory="./vectorstore"):
    """
    ChromaDBãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã™ã‚‹é–¢æ•°

    Args:
        text_chunks(list): ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        embeddings: OpenAIEmbeddingsã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        persist_directory(str): ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ãƒ‘ã‚¹

    Return:
        Chroma: ä½œæˆã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢
    """
    try:
        if os.path.exists(persist_directory):
            shutil.rmtree(persist_directory)

        vectorstore = Chroma.from_texts(
            texts=text_chunks, embedding=embeddings, persist_directory=persist_directory
        )

        vectorstore.persist()

        return vectorstore

    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None


def main():
    """
    Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """

    # NOTE: ãƒšãƒ¼ã‚¸ã®metaæƒ…å ±è¨­å®š
    st.set_page_config(
        page_title="ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„QAãƒœãƒƒãƒˆ", page_icon="ğŸ¤–", layout="wide"
    )

    st.title("ğŸ¤– ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„QAãƒœãƒƒãƒˆ")
    st.markdown("ç¤¾å†…ã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ï¼")

    with st.sidebar:
        st.header("è¨­å®š")
        st.info("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")

        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            st.success("âœ… APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
        else:
            st.error("âŒ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        st.divider()

        st.subheader("ğŸ“„ PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        uploaded_file = st.file_uploader(
            "ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type="pdf",
            help="ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„ãŒè¨˜è¼‰ã•ã‚ŒãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
        )

        st.subheader("âš™ï¸ ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²è¨­å®š")
        chunk_size = st.slider(
            "ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º",
            min_value=500,
            max_value=2000,
            value=1000,
            step=100,
            help="å„ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§æ–‡å­—æ•°",
        )

        chunk_overlap = st.slider(
            "ãƒãƒ£ãƒ³ã‚¯é‡è¤‡",
            min_value=0,
            max_value=500,
            value=200,
            step=50,
            help="éš£æ¥ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã®é‡è¤‡æ–‡å­—æ•°",
        )

        if uploaded_file is not None:
            with st.spinner("PDFã‚’èª­ã¿è¾¼ã¿ä¸­"):
                extract_text = extract_text_from_pdf(uploaded_file)

                if extract_text:
                    text_chunks = split_text_into_chunks(
                        extract_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap
                    )

                    if text_chunks:
                        st.success("âœ… PDFèª­ã¿è¾¼ã¿å®Œäº†")

                        # NOTE: Streamlitã¯å†å®Ÿè¡Œå‹ã®ãŸã‚ã€PDFãƒ†ã‚­ã‚¹ãƒˆã‚„chunkãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒã—ã¦å†æç”»æ™‚ã‚‚ç¶­æŒ
                        st.session_state.pdf_text = extract_text
                        st.session_state.text_chunks = text_chunks

                        st.info(
                            f"""
                        ğŸ“Š **å‡¦ç†çµæœ:**
                        - ç·æ–‡å­—æ•°: {len(extract_text):,}
                        - ãƒãƒ£ãƒ³ã‚¯æ•°: {len(text_chunks)}
                        - æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°: {count_tokens(extract_text):,}
                                """
                        )

                        with st.expander("ğŸ“– ãƒãƒ£ãƒ³ã‚¯ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
                            for i, chunk in enumerate(text_chunks[:3]):
                                st.write(f"**ãƒãƒ£ãƒ³ã‚¯ {i+1}**")
                                st.text_area(
                                    f"ãƒãƒ£ãƒ³ã‚¯{i+1}ã®å†…å®¹",
                                    chunk[:300] + "..." if len(chunk) > 300 else chunk,
                                    height=100,
                                    disabled=True,
                                    key=f"chunk_{i}",
                                )

                            if len(text_chunks) > 3:
                                st.info(f"... ä»– {len(text_chunks) - 3} å€‹ã®ãƒãƒ£ãƒ³ã‚¯")

    st.subheader("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    if "text_chunks" not in st.session_state:
        st.warning("ğŸ“„ ã¾ãšã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        return

    embeddings = initialize_embeddings()
    if not embeddings:
        st.error("âŒ EmbeddingsåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    user_question = st.text_input(
        "è³ªå•:", placeholder="ä¾‹: pythonã®å¤‰æ•°å‘½åè¦å‰‡ã‚’æ•™ãˆã¦"
    )

    if st.button("è³ªå•ã™ã‚‹"):
        if user_question:
            st.info(f"è³ªå•: {user_question}")

            with st.spinner("ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿæ–½ä¸­..."):
                try:
                    # NOTE: è³ªå•æ–‡ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                    question_embeddig = embeddings.embed_query(user_question)

                    # NOTE: ï¼ˆé–‹ç™ºä¸­ï¼‰ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¯¾è±¡ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å…ˆé ­10å€‹ã«åˆ¶é™
                    chunks = st.session_state.text_chunks[:10]
                    # NOTE: PDFã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                    chunk_embeddings = embeddings.embed_documents(chunks)

                    st.success("âœ… ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†")
                    st.info(f"æ¤œç´¢å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
                    st.warning(
                        "âš ï¸ ã¾ã ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¯å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆæ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å®Ÿè£…ã—ã¾ã™ï¼‰"
                    )

                except Exception as e:
                    st.error(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

        else:
            st.error("è³ªå•ã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    main()
