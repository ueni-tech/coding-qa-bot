import gc
import os
import shutil
from pathlib import Path

import PyPDF2
import streamlit as st
import tiktoken
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from chromadb.config import Settings
from langchain_community.vectorstores import Chroma

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. å®šæ•°ãƒ»è¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

PERSIST_DIR = Path(__file__).parent / "vectorstore"

CHROMA_SETTINGS = Settings(
    allow_reset=True,  # reset() ã‚’è¨±å¯ï¼ˆé–‹ç™ºç”¨ï¼‰
    is_persistent=True,  # æ°¸ç¶šãƒ¢ãƒ¼ãƒ‰
    persist_directory=str(PERSIST_DIR),
    anonymized_telemetry=False,  # ãƒ†ãƒ¬ãƒ¡ãƒˆãƒªç„¡åŠ¹
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def extract_text_from_pdf(pdf_file):
    reader = PyPDF2.PdfReader(pdf_file)
    return "\n".join((page.extract_text() or "") for page in reader.pages)


@st.cache_data
def split_text(text: str, size=1000, overlap=200):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=size,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", " ", ""],
        length_function=len,
    )
    return splitter.split_text(text)


def count_tokens(text: str, model="gpt-3.5-turbo"):
    enc = tiktoken.encoding_for_model(model)
    return len(enc.encode(text))


@st.cache_resource
def init_embeddings():
    return OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. VectorStore æ“ä½œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def cleanup_dirs(keep_uuid: str | None):
    """persist_directory é…ä¸‹ã§ keep_uuid ä»¥å¤–ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤"""
    if not PERSIST_DIR.exists():
        return
    for p in PERSIST_DIR.iterdir():
        if p.is_dir() and p.name != keep_uuid:
            shutil.rmtree(p)


def rebuild_vectorstore(chunks, embeds):
    """
    æ—¢å­˜æ¥ç¶šã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†æ§‹ç¯‰ã€‚
    1) æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º / reset
    2) æ–°è¦ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
    3) å¤ã„ UUID ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤
    """
    # â‘  æ—¢å­˜æ¥ç¶šã‚’å®‰å…¨ã«é–‰ã˜ã‚‹
    vs_old = st.session_state.pop("vectorstore", None)
    if vs_old:
        try:
            vs_old._client.reset()  # allow_reset=True å¿…é ˆ
        except Exception:
            pass
        del vs_old
        gc.collect()

    # â‘¡ æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ç”Ÿæˆ
    vs_new = Chroma.from_texts(
        texts=chunks,
        embedding=embeds,
        client_settings=CHROMA_SETTINGS,
    )
    vs_new.persist()
    st.session_state.vectorstore = vs_new

    # â‘¢ å¤ããªã£ãŸ UUID ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤
    current_uuid = vs_new._collection.id  # ä»Šä½¿ã£ã¦ã„ã‚‹ UUID
    cleanup_dirs(current_uuid)

    return vs_new


def load_vectorstore(embeds):
    if PERSIST_DIR.exists():
        return Chroma(
            persist_directory=str(PERSIST_DIR),
            embedding_function=embeds,
            client_settings=CHROMA_SETTINGS,
        )
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    st.set_page_config(
        page_title="ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„QAãƒœãƒƒãƒˆ", page_icon="ğŸ¤–", layout="wide"
    )
    st.title("ğŸ¤– ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„QAãƒœãƒƒãƒˆ")
    embeds = init_embeddings()
    if not embeds:
        st.stop()

    # æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢èª­ã¿è¾¼ã¿
    if vs := load_vectorstore(embeds):
        st.session_state.vectorstore = vs
        st.success("æ—¢å­˜ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ âœ…")

    # â”€â”€ ã‚µã‚¤ãƒ‰ãƒãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.sidebar:
        st.header("è¨­å®š")
        pdf = st.file_uploader("ğŸ“„ è¦ç´„ PDF", type="pdf")
        size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", 500, 2000, 1000, 100)
        over = st.slider("ãƒãƒ£ãƒ³ã‚¯é‡è¤‡", 0, 500, 200, 50)
        top_k = st.slider("æ¤œç´¢ä»¶æ•°", 1, 10, 3)

        if st.button("ğŸ’£ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒªã‚»ãƒƒãƒˆ"):
            # æ—¢å­˜æ¥ç¶šã®ã¿ã‚’ reset ã—ã€ãƒ•ã‚©ãƒ«ãƒ€æƒé™¤ã‚‚è¡Œã†
            if "vectorstore" in st.session_state:
                uuid_now = st.session_state.vectorstore._collection.id
                st.session_state.vectorstore._client.reset()
                cleanup_dirs(uuid_now)  # ç©ºã ãŒä»Šä½¿ã† UUID ã¯æ®‹ã™
                del st.session_state.vectorstore
            st.success("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
            st.rerun()

    # â”€â”€ PDF ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if pdf:
        with st.spinner("PDF è§£æä¸­..."):
            text = extract_text_from_pdf(pdf)
            chunks = split_text(text, size, over)
            rebuild_vectorstore(chunks, embeds)
            st.success(
                f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å†æ§‹ç¯‰å®Œäº† | æ–‡å­—æ•° {len(text):,} | "
                f"ãƒãƒ£ãƒ³ã‚¯ {len(chunks)} | æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³ {count_tokens(text):,}"
            )

    # â”€â”€ è³ªå• UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("è³ªå•")
    if "vectorstore" not in st.session_state:
        st.info("ã¾ãš PDF ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        return

    q = st.text_input("è³ªå•ã‚’å…¥åŠ›", placeholder="ä¾‹: Python ã®å¤‰æ•°å‘½åè¦å‰‡ã¯ï¼Ÿ")
    if st.button("æ¤œç´¢"):
        if not q:
            st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            return
        with st.spinner("æ¤œç´¢ä¸­â€¦"):
            res = st.session_state.vectorstore.similarity_search_with_score(q, k=top_k)
        if not res:
            st.info("é–¢é€£æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        for i, (doc, score) in enumerate(res, 1):
            with st.expander(f"{i}. score {score:.3f}"):
                st.write(doc.page_content)


if __name__ == "__main__":
    main()
