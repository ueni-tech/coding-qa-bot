"""
ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
ãƒ¡ã‚¤ãƒ³ã®ãƒãƒ£ãƒƒãƒˆUIã‚’ç®¡ç†
"""

import streamlit as st
from typing import Any
from app.controllers import rag_controller
from config.prompts import ERROR_MESSAGES
from views.components.sidebar import render_system_info


def render_chat_interface(embeddings: Any, llm: Any, top_k: int):
    """
    ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°

    Args:
        embeddings: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        llm: LLMãƒ¢ãƒ‡ãƒ«
        top_k: æ¤œç´¢ä»¶æ•°
    """
    st.subheader("ğŸ’¬ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

    if "vectorstore" not in st.session_state:
        _render_empty_state()
        return

    col1, col2 = st.columns([4, 1])

    with col1:
        question = st.text_input(
            "è³ªå•",
            placeholder="ä¾‹: ã‚¯ãƒ©ã‚¹ã®å‘½åè¦å‰‡ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            key="question_input",
            label_visibility="collapsed",
        )

    with col2:
        mode = st.radio(
            "ãƒ¢ãƒ¼ãƒ‰",
            options=["ğŸ¤– RAGå›ç­”", "ğŸ” æ¤œç´¢ã®ã¿"],
            index=0,
            key="search_mode",
            label_visibility="collapsed",
        )

    col1, col2, col3 = st.columns([2, 1, 2])
    with col2:
        execute_button = st.button(
            "å®Ÿè¡Œ", type="primary", use_container_width=True, disabled=not question
        )

    if execute_button and question:
        _process_question(
            question=question,
            embeddings=embeddings,
            llm=llm,
            top_k=top_k,
            mode="search_only" if "æ¤œç´¢ã®ã¿" in mode else "rag",
        )

    if st.session_state.get("show_history", False):
        _render_history()


def _render_empty_state():
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ç©ºçŠ¶æ…‹è¡¨ç¤º"""
    st.warning(ERROR_MESSAGES["no_pdf"])

    with st.expander("ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰", expanded=True):
        st.markdown(
            """
        ### ä½¿ç”¨æ–¹æ³•
        
        1. **ğŸ“ PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™**
            - ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¾ã™
        
        2. **â¬†ï¸ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**
            - å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„ PDFã€ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
        
        3. **âš™ï¸ è¨­å®šèª¿æ•´**ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†å‰²å˜ä½
            - æ¤œç´¢ä»¶æ•°: å›ç­”ç”Ÿæˆæ™‚ã«å‚ç…§ã™ã‚‹æ–‡æ›¸æ•°
        
        4. **ğŸ’¬ è³ªå•ã™ã‚‹**
            - ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«è³ªå•ã‚’å…¥åŠ›
            - ã€Œå®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
        
        ### è³ªå•ä¾‹
        - ã€Œå¤‰æ•°ã®å‘½åè¦å‰‡ã¯ï¼Ÿã€
        - ã€Œé–¢æ•°ã®ã‚³ãƒ¡ãƒ³ãƒˆã¯ã©ã®ã‚ˆã†ã«æ›¸ãã¹ãï¼Ÿã€
        - ã€Œã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã¯ï¼Ÿã€
        """
        )


def _process_question(question: str, embeddings: Any, llm: Any, top_k: int, mode: str):
    """
    è³ªå•ã‚’å‡¦ç†ã™ã‚‹çµ±åˆé–¢æ•°

    Args:
        question: è³ªå•æ–‡
        embeddings: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        llm: LLMãƒ¢ãƒ‡ãƒ«
        top_k: æ¤œç´¢ä»¶æ•°
        mode: "rag" ã¾ãŸã¯ "search_only"
    """
    if not question.strip():
        st.warning(ERROR_MESSAGES["no_question"])
        return

    with st.spinner("ğŸ”„ å‡¦ç†ä¸­..." if mode == "rag" else "ğŸ” æ¤œç´¢ä¸­..."):
        result = rag_controller.search_and_answer(
            question=question,
            vectorstore=st.session_state.vectorstore,
            llm=llm or st.session_state.get("llm"),
            top_k=top_k,
            mode=mode,
        )

    if result["success"]:
        _display_results(result, question)
        _add_to_history(question, result)
    else:
        _display_error(result["error"])


def _display_results(result: dict, question: str):
    """
    æ¤œç´¢/å›ç­”çµæœã‚’è¡¨ç¤º

    Args:
        result: å‡¦ç†çµæœ
        question: å…ƒã®è³ªå•
    """
    if result["mode"] == "rag":
        st.markdown("### ğŸ¤– AIå›ç­”")

        with st.container():
            st.markdown(result["answer"])

        _display_reference_documents(result["documents"])

    else:
        _display_search_results(result["documents"])


def _display_reference_documents(documents: list):
    """å‚ç…§æ–‡æ›¸ã‚’è¡¨ç¤º"""
    if not documents:
        return

    with st.expander(f"ğŸ“š å‚ç…§ã—ãŸæ–‡æ›¸ ({len(documents)}ä»¶)", expanded=False):
        for i, doc in enumerate(documents, 1):
            st.markdown(f"**æ–‡æ›¸ {i}**")

            # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦‹ã‚„ã™ãè¡¨ç¤ºï¼ˆ500æ–‡å­—ã§åˆ‡ã‚Šè©°ã‚ï¼‰
            content = doc.page_content
            display_content = content[:500] + "..." if len(content) > 500 else content

            with st.container():
                st.text(display_content)

            if i < len(documents):
                st.divider()


def _display_search_results(documents: list):
    """æ¤œç´¢çµæœã‚’è¡¨ç¤º"""
    st.markdown("### ğŸ” æ¤œç´¢çµæœ")

    if not documents:
        st.info(ERROR_MESSAGES["no_results"])
        return

    st.info(f"ğŸ“„ {len(documents)}ä»¶ã®é–¢é€£æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

    for i, doc in enumerate(documents, 1):
        with st.expander(f"ğŸ“„ æ–‡æ›¸ {i}", expanded=(i == 1)):
            st.text(doc.page_content)


def _display_error(error_message: str):
    """ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º"""
    st.error(f"âŒ ã‚¨ãƒ©ãƒ¼: {error_message}")

    with st.expander("ğŸ†˜ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", expanded=False):
        st.markdown(
            """
        ### ã‚ˆãã‚ã‚‹ã‚¨ãƒ©ãƒ¼ã¨å¯¾å‡¦æ³•
        
        **1. APIé–¢é€£ã®ã‚¨ãƒ©ãƒ¼**
        - OpenAI APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        - APIã®åˆ©ç”¨åˆ¶é™ã«é”ã—ã¦ã„ãªã„ã‹ç¢ºèª
        - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª
        
        **2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢é–¢é€£ã®ã‚¨ãƒ©ãƒ¼**
        - PDFãŒæ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        - ã€Œãƒªã‚»ãƒƒãƒˆã€ãƒœã‚¿ãƒ³ã§åˆæœŸåŒ–ã—ã¦ã‹ã‚‰å†è©¦è¡Œ
        
        **3. ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®ã‚¨ãƒ©ãƒ¼**
        - é¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
        - ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ›´æ–°ã—ã¦ã‹ã‚‰å†è©¦è¡Œ
        
        **ãã‚Œã§ã‚‚è§£æ±ºã—ãªã„å ´åˆ**
        - ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ãƒªãƒ­ãƒ¼ãƒ‰
        - ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å†èµ·å‹•
        """
        )


def _add_to_history(question: str, result: dict):
    """
    è³ªå•ã¨çµæœã‚’å±¥æ­´ã«è¿½åŠ 

    Args:
        question: è³ªå•
        result: çµæœ
    """
    if "history" not in st.session_state:
        st.session_state.history = []

    history_entry = {
        "question": question,
        "mode": result["mode"],
        "answer": result.get("answer", ""),
        "document_count": len(result.get("documents", [])),
    }

    st.session_state.history.append(history_entry)

    # å±¥æ­´ã¯æœ€æ–°10ä»¶ã¾ã§ä¿æŒ
    if len(st.session_state.history) > 10:
        st.session_state.history = st.session_state.history[-10:]


def _render_history():
    """è³ªå•å±¥æ­´ã‚’è¡¨ç¤º"""
    if "history" not in st.session_state or not st.session_state.history:
        return

    with st.expander("ğŸ“œ è³ªå•å±¥æ­´", expanded=False):
        for i, entry in enumerate(reversed(st.session_state.history)):
            history_index = len(st.session_state.history) - i
            st.markdown(f"**Q{history_index}**: {entry['question']}")

            if entry["mode"] == "rag" and entry["answer"]:
                # å›ç­”ã‚’200æ–‡å­—ã§åˆ‡ã‚Šè©°ã‚ã¦è¡¨ç¤º
                truncated_answer = (
                    entry["answer"][:200] + "..."
                    if len(entry["answer"]) > 200
                    else entry["answer"]
                )
                st.markdown(f"**A**: {truncated_answer}")

            st.caption(f"å‚ç…§æ–‡æ›¸: {entry['document_count']}ä»¶")
            st.divider()
